{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8e82ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.4, Python Version: 3.6.13, Platform: Linux-4.14.252-131.483.amzn1.x86_64-x86_64-with-glibc2.9\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    }
   ],
   "source": [
    "##nodejs:  https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-up-node-on-ec2-instance.html\n",
    "\n",
    "# !pip install \"jupyterlab>=3\" \"ipywidgets>=7.6\"\n",
    "# !pip install jupyter-dash\n",
    "# !jupyter lab build\n",
    "\n",
    "\n",
    "# !pip install snowflake --user\n",
    "# !pip install snowflake-connector-python --user\n",
    "# !pip install category_encoders\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm --user\n",
    "import os\n",
    "import sys\n",
    "path=!pwd\n",
    "# sys.path.append(os.path.join(path[0], '..'))\n",
    "# sys.path.append('/home/ec2-user/SageMaker/jupyter-notebooks/')\n",
    "# from utils import *\n",
    "import snowflake.connector\n",
    "from datetime import timedelta\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "import logging \n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from category_encoders import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import r2_score as r2_score\n",
    "import sklearn.model_selection\n",
    "\n",
    "from io import StringIO\n",
    "class Utils():\n",
    "    @staticmethod\n",
    "    def to_csv_s3(content, bucket, key_path, filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        csv_buffer = StringIO()\n",
    "        content.to_csv(csv_buffer)\n",
    "        client.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n",
    "        logger.info(f'Saved to {bucket}/{key}')\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_pkl_s3(content, bucket, key_path, filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        obj = pickle.dumps(content)\n",
    "        client.put_object(Bucket=bucket, Key=key, Body=obj)\n",
    "        logger.info(f'Saved model to {os.path.join(bucket, key)}')\n",
    "        logger.info(f'Saved to {bucket}/{key}')\n",
    "\n",
    "    @staticmethod\n",
    "    def read_csv_s3(bucket, key_path,filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        logger.info(f'Reading from {bucket}/{key}')\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        df = pd.read_csv(obj['Body'], na_values=\"\\\\N\")\n",
    "        return df\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_pkl_s3(bucket, key_path,filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        logger.info(f'Reading from {bucket}/{key}')\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        body = obj['Body'].read()\n",
    "        model = pickle.loads(body)\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "class Credentials(metaclass=ABCMeta):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "class SSMPSCredentials(Credentials):\n",
    "    def __init__(self, secretid: str):\n",
    "        self._secretid = secretid\n",
    "        self._secrets = {}\n",
    "        \n",
    "    def get_keys(self):\n",
    "        \"\"\"\n",
    "        credential fetching \n",
    "        \"\"\"\n",
    "        _aws_sm_args = {'service_name': 'secretsmanager', 'region_name': 'us-east-1'}\n",
    "        secrets_client = boto3.client(**_aws_sm_args)\n",
    "        get_secret_value_response = secrets_client.get_secret_value(SecretId=self._secretid)\n",
    "        return get_secret_value_response\n",
    "    \n",
    "    \n",
    "class BaseConnector(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def connect(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class SnowflakeConnector(BaseConnector):\n",
    "    def __init__(self, credentials: Credentials):\n",
    "        keys = credentials.get_keys()\n",
    "        self._secrets = json.loads(keys.get('SecretString', \"{}\"))\n",
    "\n",
    "    def connect(self, dbname: str, schema: str = 'DEFAULT'):\n",
    "        ctx = snowflake.connector.connect(\n",
    "            user=self._secrets['login_name'],\n",
    "            password=self._secrets['login_password'],\n",
    "            account=self._secrets['account'],\n",
    "            warehouse=self._secrets['warehouse'],\n",
    "            database=dbname,\n",
    "            schema=schema\n",
    "        )\n",
    "\n",
    "        return ctx\n",
    "\n",
    "\n",
    "def run_query(query, dbname, schema):\n",
    "    SF_CREDS = 'datascience-max-dev-sagemaker-notebooks'\n",
    "\n",
    "    conn=SnowflakeConnector(SSMPSCredentials(SF_CREDS))\n",
    "    ctx=conn.connect(dbname,schema)\n",
    "    cursor = ctx.cursor()\n",
    "    cursor.execute(query)\n",
    "    df = pd.DataFrame(cursor.fetchall(), columns = [desc[0] for desc in cursor.description])\n",
    "    df.columns= df.columns.str.lower()\n",
    "    return df    \n",
    "\n",
    "## Credentials\n",
    "SF_CREDS = 'datascience-max-dev-sagemaker-notebooks'\n",
    "\n",
    "## Snowflake connection \n",
    "conn=SnowflakeConnector(SSMPSCredentials(SF_CREDS))\n",
    "ctx=conn.connect(\"MAX_PROD\",\"DATASCIENCE_STAGE\")\n",
    "cur = ctx.cursor()\n",
    "\n",
    "def cvdf_to_snowflake(df, table_name):\n",
    "    stage = '@HBO_OUTBOUND_DATASCIENCE_CONTENT_DEV'\n",
    "    output_bucket = \"hbo-outbound-datascience-content-dev\"\n",
    "    dbname, schema = 'MAX_DEV', 'WORKSPACE'\n",
    "    \n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index = False)\n",
    "    content = csv_buffer.getvalue()\n",
    "    client = boto3.client('s3')\n",
    "    client.put_object(Bucket=output_bucket, Key=table_name, Body=content)\n",
    "\n",
    "    print ('Create Table: ' + table_name)\n",
    " \n",
    "    run_query('''\n",
    "    create or replace table {table_name} (\n",
    "    title_name varchar,\n",
    "    first_view_month varchar,\n",
    "    premiere_date varchar,\n",
    "    season_number int, \n",
    "    tier varchar,\n",
    "    content_category  varchar,\n",
    "    category varchar,\n",
    "    prequel_count int,\n",
    "    prequel_featured_count int,\n",
    "    prequel_featured_count_s int,\n",
    "    page_views int,\n",
    "    page_views_s int,\n",
    "    tier_adjusted int,\n",
    "    first_views_pred float,\n",
    "    model_train_date varchar,\n",
    "    schedule_label varchar\n",
    "    )\n",
    "    '''.format(table_name = table_name), dbname, schema)\n",
    "\n",
    "    print ('Begin Uploading')\n",
    "    run_query('''\n",
    "    insert into max_dev.workspace.{table_name}\n",
    "\n",
    "    select \n",
    "              $1\n",
    "            , $2\n",
    "            , $3\n",
    "            , $4\n",
    "            , $5\n",
    "            , $6\n",
    "            , $7\n",
    "            , $8\n",
    "            , $9\n",
    "            , $10\n",
    "            , $11\n",
    "            , $12\n",
    "            , $13\n",
    "            , $14\n",
    "            , $15\n",
    "            , $17\n",
    "    from {stage}/psi_first_views/psi_monthly_xgb_forecast.csv\n",
    "\n",
    "     (FILE_FORMAT => csv_v2)\n",
    "\n",
    "    '''.format(stage = stage, table_name = table_name,\n",
    "              file_name = table_name+'.csv')\n",
    "            , dbname, schema)\n",
    "\n",
    "    print ('Finish Uploading')   \n",
    "    \n",
    "    \n",
    "import io\n",
    "\n",
    "output_bucket = 'hbo-outbound-datascience-content-dev'\n",
    "key_path = 'psi_first_views'\n",
    "\n",
    "# Utils.read_csv_s3(output_bucket, key_path, f'psi_monthly_xgb_forecast.csv')\n",
    "df_pred_future_out = df_pred_future_out.reset_index()\n",
    "cvdf_to_snowflake(df_pred_future_out, 'firstview_postgl_temp')\n",
    "\n",
    "#  hbo-outbound-datascience-content-dev/psi_first_views/psi_monthly_xgb_forecast.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

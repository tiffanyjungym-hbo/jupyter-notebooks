{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fec8429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.4, Python Version: 3.6.13, Platform: Linux-4.14.299-152.520.amzn1.x86_64-x86_64-with-glibc2.9\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.4, Python Version: 3.6.13, Platform: Linux-4.14.299-152.520.amzn1.x86_64-x86_64-with-glibc2.9\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Table: future_title_imdb_map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query: [create or replace table future_title_imdb_map( title_name varchar, tier int, sea...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.4, Python Version: 3.6.13, Platform: Linux-4.14.299-152.520.amzn1.x86_64-x86_64-with-glibc2.9\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Uploading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query: [insert into max_dev.workspace.future_title_imdb_map  select $1, $2, $3, $4, $5, ...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Uploading\n"
     ]
    }
   ],
   "source": [
    "##nodejs:  https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-up-node-on-ec2-instance.html\n",
    "\n",
    "# !pip install \"jupyterlab>=3\" \"ipywidgets>=7.6\"\n",
    "# !pip install jupyter-dash\n",
    "# !jupyter lab build\n",
    "\n",
    "\n",
    "# !pip install snowflake --user\n",
    "# !pip install snowflake-connector-python --user\n",
    "# !pip install category_encoders\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm --user\n",
    "import os\n",
    "import sys\n",
    "path=!pwd\n",
    "sys.path.append(os.path.join(path[0], '..'))\n",
    "sys.path.append('/home/ec2-user/SageMaker/jupyter-notebooks/')\n",
    "from utils import *\n",
    "import snowflake.connector\n",
    "from datetime import timedelta\n",
    "import json\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import boto3\n",
    "import logging \n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from category_encoders import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "# import lightgbm as lgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import r2_score as r2_score\n",
    "import sklearn.model_selection\n",
    "\n",
    "class Credentials(metaclass=ABCMeta):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "class SSMPSCredentials(Credentials):\n",
    "    def __init__(self, secretid: str):\n",
    "        self._secretid = secretid\n",
    "        self._secrets = {}\n",
    "        \n",
    "    def get_keys(self):\n",
    "        \"\"\"\n",
    "        credential fetching \n",
    "        \"\"\"\n",
    "        _aws_sm_args = {'service_name': 'secretsmanager', 'region_name': 'us-east-1'}\n",
    "        secrets_client = boto3.client(**_aws_sm_args)\n",
    "        get_secret_value_response = secrets_client.get_secret_value(SecretId=self._secretid)\n",
    "        return get_secret_value_response\n",
    "    \n",
    "    \n",
    "class BaseConnector(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def connect(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class SnowflakeConnector(BaseConnector):\n",
    "    def __init__(self, credentials: Credentials):\n",
    "        keys = credentials.get_keys()\n",
    "        self._secrets = json.loads(keys.get('SecretString', \"{}\"))\n",
    "\n",
    "    def connect(self, dbname: str, schema: str = 'DEFAULT'):\n",
    "        ctx = snowflake.connector.connect(\n",
    "            user=self._secrets['login_name'],\n",
    "            password=self._secrets['login_password'],\n",
    "            account=self._secrets['account'],\n",
    "            warehouse=self._secrets['warehouse'],\n",
    "            database=dbname,\n",
    "            schema=schema\n",
    "        )\n",
    "\n",
    "        return ctx\n",
    "\n",
    "\n",
    "def run_query(query, dbname, schema):\n",
    "    SF_CREDS = 'datascience-max-dev-sagemaker-notebooks'\n",
    "\n",
    "    conn=SnowflakeConnector(SSMPSCredentials(SF_CREDS))\n",
    "    ctx=conn.connect(dbname,schema)\n",
    "    cursor = ctx.cursor()\n",
    "    cursor.execute(query)\n",
    "    df = pd.DataFrame(cursor.fetchall(), columns = [desc[0] for desc in cursor.description])\n",
    "    df.columns= df.columns.str.lower()\n",
    "    return df    \n",
    "## Credentials\n",
    "SF_CREDS = 'datascience-max-dev-sagemaker-notebooks'\n",
    "\n",
    "## Snowflake connection \n",
    "conn=SnowflakeConnector(SSMPSCredentials(SF_CREDS))\n",
    "ctx=conn.connect(\"MAX_PROD\",\"DATASCIENCE_STAGE\")\n",
    "cur = ctx.cursor()\n",
    "\n",
    "def cvdf_to_snowflake(df, table_name):\n",
    "    stage = '@HBO_OUTBOUND_DATASCIENCE_CONTENT_DEV'\n",
    "    output_bucket = \"hbo-outbound-datascience-content-dev\"\n",
    "    filename ='psi/' + table_name + '.csv'\n",
    "    dbname, schema = 'MAX_DEV', 'workspace'\n",
    "    \n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index = False)\n",
    "    content = csv_buffer.getvalue()\n",
    "    client = boto3.client('s3')\n",
    "    client.put_object(Bucket=output_bucket, Key=filename, Body=content)\n",
    "\n",
    "    print ('Create Table: ' + table_name)\n",
    "    run_query('''\n",
    "    create or replace table {table_name}(\n",
    "    title_name varchar,\n",
    "    tier int,\n",
    "    season_number int, \n",
    "    category varchar,\n",
    "    effective_start_date varchar,\n",
    "    imdb_title_name varchar,\n",
    "    imdb_title_id varchar,\n",
    "    content_category varchar\n",
    "    )\n",
    "    '''.format(table_name = table_name), dbname, schema)\n",
    "\n",
    "    print ('Begin Uploading')\n",
    "    run_query('''\n",
    "    insert into max_dev.workspace.{table_name}\n",
    "\n",
    "    select \n",
    "          $1, $2, $3, $4, $5, $6, $7, $8\n",
    "    from {stage}/psi/{file_name}\n",
    "    \n",
    "     (FILE_FORMAT => CSV_ED)\n",
    "\n",
    "    '''.format(stage = stage, table_name = table_name,\n",
    "              file_name = table_name+'.csv')\n",
    "            , dbname, schema)\n",
    "\n",
    "    print ('Finish Uploading')   \n",
    "    \n",
    "    \n",
    "class Utils():\n",
    "    @staticmethod\n",
    "    def to_csv_s3(content, bucket, key_path, filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        csv_buffer = StringIO()\n",
    "        content.to_csv(csv_buffer)\n",
    "        client.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n",
    "        logger.info(f'Saved to {bucket}/{key}')\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_pkl_s3(content, bucket, key_path, filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        obj = pickle.dumps(content)\n",
    "        client.put_object(Bucket=bucket, Key=key, Body=obj)\n",
    "        logger.info(f'Saved model to {os.path.join(bucket, key)}')\n",
    "        logger.info(f'Saved to {bucket}/{key}')\n",
    "\n",
    "    @staticmethod\n",
    "    def read_csv_s3(bucket, key_path,filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        logger.info(f'Reading from {bucket}/{key}')\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        df = pd.read_csv(obj['Body'], na_values=\"\\\\N\")\n",
    "        return df\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_pkl_s3(bucket, key_path,filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        logger.info(f'Reading from {bucket}/{key}')\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        body = obj['Body'].read()\n",
    "        model = pickle.loads(body)\n",
    "        return model\n",
    "\n",
    "    \n",
    "# df_fp.tier = df_fp.tier.astype('int')\n",
    "cvdf_to_snowflake(df_fp, 'future_title_imdb_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314d773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b08a662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_name</th>\n",
       "      <th>tier</th>\n",
       "      <th>season_number</th>\n",
       "      <th>category</th>\n",
       "      <th>effective_start_date</th>\n",
       "      <th>imdb_title_name</th>\n",
       "      <th>imdb_title_id</th>\n",
       "      <th>content_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Bridge</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Unscripted Series</td>\n",
       "      <td>2021-02-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt13027548</td>\n",
       "      <td>series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Westworld</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Scripted Drama Series</td>\n",
       "      <td>2022-06-26</td>\n",
       "      <td>Westworld S4</td>\n",
       "      <td>tt0475784</td>\n",
       "      <td>series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tig Notaro: Drawn</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Specials</td>\n",
       "      <td>2021-07-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>special</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Odo</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Kids &amp; Family</td>\n",
       "      <td>2022-04-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt16538364</td>\n",
       "      <td>series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The First Year</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Documentary Features</td>\n",
       "      <td>2022-07-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title_name  tier  season_number               category  \\\n",
       "0         The Bridge     3              1      Unscripted Series   \n",
       "1          Westworld     1              4  Scripted Drama Series   \n",
       "2  Tig Notaro: Drawn     3              0               Specials   \n",
       "3                Odo     3              3          Kids & Family   \n",
       "4     The First Year     3              0   Documentary Features   \n",
       "\n",
       "  effective_start_date imdb_title_name imdb_title_id content_category  \n",
       "0           2021-02-11             NaN    tt13027548           series  \n",
       "1           2022-06-26    Westworld S4     tt0475784           series  \n",
       "2           2021-07-24             NaN           NaN          special  \n",
       "3           2022-04-07             NaN    tt16538364           series  \n",
       "4           2022-07-05             NaN           NaN           movies  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0737ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.4, Python Version: 3.6.13, Platform: Linux-4.14.299-152.520.amzn1.x86_64-x86_64-with-glibc2.9\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    }
   ],
   "source": [
    "##nodejs:  https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-up-node-on-ec2-instance.html\n",
    "\n",
    "# !pip install \"jupyterlab>=3\" \"ipywidgets>=7.6\"\n",
    "# !pip install jupyter-dash\n",
    "# !jupyter lab build\n",
    "\n",
    "\n",
    "# !pip install snowflake --user\n",
    "# !pip install snowflake-connector-python --user\n",
    "# !pip install category_encoders\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm --user\n",
    "import os\n",
    "import sys\n",
    "path=!pwd\n",
    "sys.path.append(os.path.join(path[0], '..'))\n",
    "sys.path.append('/home/ec2-user/SageMaker/jupyter-notebooks/')\n",
    "from utils import *\n",
    "import snowflake.connector\n",
    "from datetime import timedelta\n",
    "import json\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import boto3\n",
    "import logging \n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from category_encoders import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "# import lightgbm as lgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import r2_score as r2_score\n",
    "import sklearn.model_selection\n",
    "\n",
    "class Credentials(metaclass=ABCMeta):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "class SSMPSCredentials(Credentials):\n",
    "    def __init__(self, secretid: str):\n",
    "        self._secretid = secretid\n",
    "        self._secrets = {}\n",
    "        \n",
    "    def get_keys(self):\n",
    "        \"\"\"\n",
    "        credential fetching \n",
    "        \"\"\"\n",
    "        _aws_sm_args = {'service_name': 'secretsmanager', 'region_name': 'us-east-1'}\n",
    "        secrets_client = boto3.client(**_aws_sm_args)\n",
    "        get_secret_value_response = secrets_client.get_secret_value(SecretId=self._secretid)\n",
    "        return get_secret_value_response\n",
    "    \n",
    "    \n",
    "class BaseConnector(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def connect(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class SnowflakeConnector(BaseConnector):\n",
    "    def __init__(self, credentials: Credentials):\n",
    "        keys = credentials.get_keys()\n",
    "        self._secrets = json.loads(keys.get('SecretString', \"{}\"))\n",
    "\n",
    "    def connect(self, dbname: str, schema: str = 'DEFAULT'):\n",
    "        ctx = snowflake.connector.connect(\n",
    "            user=self._secrets['login_name'],\n",
    "            password=self._secrets['login_password'],\n",
    "            account=self._secrets['account'],\n",
    "            warehouse=self._secrets['warehouse'],\n",
    "            database=dbname,\n",
    "            schema=schema\n",
    "        )\n",
    "\n",
    "        return ctx\n",
    "\n",
    "\n",
    "def run_query(query, dbname, schema):\n",
    "    SF_CREDS = 'datascience-max-dev-sagemaker-notebooks'\n",
    "\n",
    "    conn=SnowflakeConnector(SSMPSCredentials(SF_CREDS))\n",
    "    ctx=conn.connect(dbname,schema)\n",
    "    cursor = ctx.cursor()\n",
    "    cursor.execute(query)\n",
    "    df = pd.DataFrame(cursor.fetchall(), columns = [desc[0] for desc in cursor.description])\n",
    "    df.columns= df.columns.str.lower()\n",
    "    return df    \n",
    "## Credentials\n",
    "SF_CREDS = 'datascience-max-dev-sagemaker-notebooks'\n",
    "\n",
    "## Snowflake connection \n",
    "conn=SnowflakeConnector(SSMPSCredentials(SF_CREDS))\n",
    "ctx=conn.connect(\"MAX_PROD\",\"DATASCIENCE_STAGE\")\n",
    "cur = ctx.cursor()\n",
    "\n",
    "def cvdf_to_snowflake(df, table_name):\n",
    "    stage = '@HBO_OUTBOUND_DATASCIENCE_CONTENT_DEV'\n",
    "    output_bucket = \"hbo-outbound-datascience-content-dev\"\n",
    "    filename ='psi/' + table_name + '.csv'\n",
    "    dbname, schema = 'MAX_DEV', 'CONTENT_DATASCIENCE'\n",
    "    \n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index = False)\n",
    "    content = csv_buffer.getvalue()\n",
    "    client = boto3.client('s3')\n",
    "    client.put_object(Bucket=output_bucket, Key=filename, Body=content)\n",
    "\n",
    "    print ('Create Table: ' + table_name)\n",
    "    run_query('''\n",
    "    create or replace table {table_name}(\n",
    "    title_name varchar,\n",
    "    tier int,\n",
    "    season_number int, \n",
    "    category varchar,\n",
    "    effective_start_date varchar,\n",
    "    imdb_title_name varchar,\n",
    "    imdb_title_id varchar,\n",
    "    content_category varchar\n",
    "    )\n",
    "    '''.format(table_name = table_name), dbname, schema)\n",
    "\n",
    "    print ('Begin Uploading')\n",
    "    run_query('''\n",
    "    insert into max_dev.content_datascience.{table_name}\n",
    "\n",
    "    select \n",
    "          $1, $2, $3, $4, $5, $6, $7, $8\n",
    "    from {stage}/psi/{file_name}\n",
    "\n",
    "     (FILE_FORMAT => csv_v2)\n",
    "\n",
    "    '''.format(stage = stage, table_name = table_name,\n",
    "              file_name = table_name+'.csv')\n",
    "            , dbname, schema)\n",
    "\n",
    "    print ('Finish Uploading')   \n",
    "    \n",
    "    \n",
    "class Utils():\n",
    "    @staticmethod\n",
    "    def to_csv_s3(content, bucket, key_path, filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        csv_buffer = StringIO()\n",
    "        content.to_csv(csv_buffer)\n",
    "        client.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n",
    "        logger.info(f'Saved to {bucket}/{key}')\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_pkl_s3(content, bucket, key_path, filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        obj = pickle.dumps(content)\n",
    "        client.put_object(Bucket=bucket, Key=key, Body=obj)\n",
    "        logger.info(f'Saved model to {os.path.join(bucket, key)}')\n",
    "        logger.info(f'Saved to {bucket}/{key}')\n",
    "\n",
    "    @staticmethod\n",
    "    def read_csv_s3(bucket, key_path,filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        logger.info(f'Reading from {bucket}/{key}')\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        df = pd.read_csv(obj['Body'], na_values=\"\\\\N\")\n",
    "        return df\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_pkl_s3(bucket, key_path,filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        logger.info(f'Reading from {bucket}/{key}')\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        body = obj['Body'].read()\n",
    "        model = pickle.loads(body)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "931ae132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.4, Python Version: 3.6.13, Platform: Linux-4.14.299-152.520.amzn1.x86_64-x86_64-with-glibc2.9\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Table: future_title_imdb_map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query: [create or replace table future_title_imdb_map( title_name varchar, tier int, sea...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.4, Python Version: 3.6.13, Platform: Linux-4.14.299-152.520.amzn1.x86_64-x86_64-with-glibc2.9\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Uploading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.cursor:query: [insert into max_dev.content_datascience.future_title_imdb_map  select $1, $2, $3...]\n",
      "INFO:snowflake.connector.cursor:query execution done\n",
      "INFO:snowflake.connector.connection:closed\n",
      "INFO:snowflake.connector.connection:No async queries seem to be running, deleting session\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "100431 (22000): 01aa3f06-0606-bec5-00f8-4103e83371f3: Failed to access remote file: object in invalid storage class. The file may be in Glacier or Deep Archive. Change the storage class before accessing it. File: psi/future_program_imdb_map.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ea2d1eb82586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#        'effective_start_date', 'imdb_title_name','imdb_title_id', 'content_category']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mcvdf_to_snowflake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_fp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'future_title_imdb_map'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-7a0fca4e2e45>\u001b[0m in \u001b[0;36mcvdf_to_snowflake\u001b[0;34m(df, table_name)\u001b[0m\n\u001b[1;32m    135\u001b[0m     '''.format(stage = stage, table_name = table_name,\n\u001b[1;32m    136\u001b[0m               file_name = table_name+'.csv')\n\u001b[0;32m--> 137\u001b[0;31m             , dbname, schema)\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Finish Uploading'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-7a0fca4e2e45>\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(query, dbname, schema)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/snowflake/connector/cursor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _use_ijson, _is_put_get, _raise_put_get_error, _force_put_overwrite, file_stream)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntegrityError\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_integrity_error\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mProgrammingError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             Error.errorhandler_wrapper(\n\u001b[0;32m--> 792\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m             )\n\u001b[1;32m    794\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/snowflake/connector/errors.py\u001b[0m in \u001b[0;36merrorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0merror_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         )\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhanded_over\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/snowflake/connector/errors.py\u001b[0m in \u001b[0;36mhand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/snowflake/connector/errors.py\u001b[0m in \u001b[0;36mdefault_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mdone_format_msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done_format_msg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mconnection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mcursor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProgrammingError\u001b[0m: 100431 (22000): 01aa3f06-0606-bec5-00f8-4103e83371f3: Failed to access remote file: object in invalid storage class. The file may be in Glacier or Deep Archive. Change the storage class before accessing it. File: psi/future_program_imdb_map.csv"
     ]
    }
   ],
   "source": [
    "## Upload manually entered data to snowflake \n",
    "import io\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# df_fp = Utils.read_csv_s3('hbo-ingest-datascience-content-dev', 'psi_first_views/dev','future_program_imdb_id_full_202204.csv')\n",
    "# df_fp = df_fp.rename(columns={'premiere_date':'effective_start_date',\n",
    "#                              'title_name_imdb':'imdb_title_name',\n",
    "#                              'imdb_id':'imdb_title_id',\n",
    "#                              'program_type':'content_category'})\n",
    "\n",
    "# df_fp.loc[df_fp.content_category=='movie','content_category'] = 'movies'\n",
    "# df_fp['title_id'] = 0\n",
    "# df_fp['first_views'] = 0\n",
    "# display(df_fp.head())\n",
    "# df_fp.loc[df_fp.title_name=='Aquaman and the Lost Kingdom', 'imdb_title_id'] = 'tt9663764'\n",
    "\n",
    "# Utils.to_csv_s3(df_fp, 'hbo-ingest-datascience-content-dev', 'psi_first_views/dev','future_program_imdb_id_full_202204.csv')\n",
    "# df_fp = df_fp[['title_name', 'tier', 'season_number', 'category',\n",
    "#        'effective_start_date', 'imdb_title_name','imdb_title_id', 'content_category']]\n",
    "\n",
    "cvdf_to_snowflake(df_fp, 'future_title_imdb_map')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "522e33f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_name</th>\n",
       "      <th>tier</th>\n",
       "      <th>season_number</th>\n",
       "      <th>category</th>\n",
       "      <th>effective_start_date</th>\n",
       "      <th>imdb_title_name</th>\n",
       "      <th>imdb_title_id</th>\n",
       "      <th>content_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>The Gilded Age</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Scripted Drama Series</td>\n",
       "      <td>2023-04-17</td>\n",
       "      <td>The Gilded Age S1</td>\n",
       "      <td>tt4406178</td>\n",
       "      <td>series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>The Gilded Age / Say Her Name</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Scripted Drama Series</td>\n",
       "      <td>2024-04-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>The Gilded Age</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Scripted Drama Series</td>\n",
       "      <td>2022-01-24</td>\n",
       "      <td>The Gilded Age S1</td>\n",
       "      <td>tt4406178</td>\n",
       "      <td>series</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        title_name  tier  season_number  \\\n",
       "149                 The Gilded Age     1              2   \n",
       "534  The Gilded Age / Say Her Name     1              3   \n",
       "768                 The Gilded Age     1              1   \n",
       "\n",
       "                  category effective_start_date    imdb_title_name  \\\n",
       "149  Scripted Drama Series           2023-04-17  The Gilded Age S1   \n",
       "534  Scripted Drama Series           2024-04-15                NaN   \n",
       "768  Scripted Drama Series           2022-01-24  The Gilded Age S1   \n",
       "\n",
       "    imdb_title_id content_category  \n",
       "149     tt4406178           series  \n",
       "534           NaN           series  \n",
       "768     tt4406178           series  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fp[df_fp.title_name.str.contains('Gilded')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

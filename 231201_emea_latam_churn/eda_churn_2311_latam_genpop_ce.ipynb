{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42ebc2a5-61e4-4f2a-b1e8-450fe4436de0",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "path=!pwd\n",
    "sys.path.append(os.path.join(path[0], '..'))\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "import json\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import boto3\n",
    "# import snowflake.connector\n",
    "from io import StringIO\n",
    "import logging \n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger.info(f'Starting Notebook')\n",
    "\n",
    "from utils import *\n",
    "\n",
    "class Utils():\n",
    "    @staticmethod\n",
    "    def to_csv_s3(content, bucket, key_path, filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        csv_buffer = StringIO()\n",
    "        content.to_csv(csv_buffer)\n",
    "        client.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n",
    "        logger.info(f'Saved to {bucket}/{key}')\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_pkl_s3(content, bucket, key_path, filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        obj = pickle.dumps(content)\n",
    "        client.put_object(Bucket=bucket, Key=key, Body=obj)\n",
    "        logger.info(f'Saved to {bucket}/{key}')\n",
    "\n",
    "    @staticmethod\n",
    "    def read_csv_s3(bucket, key_path,filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        df = pd.read_csv(obj['Body'], na_values=\"\\\\N\")\n",
    "        logger.info(f'Read from {bucket}/{key}')\n",
    "        return df\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_pkl_s3(bucket, key_path,filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        body = obj['Body'].read()\n",
    "        model = pickle.loads(body)\n",
    "        logger.info(f'Read from {bucket}/{key}')\n",
    "        return model    \n",
    "\n",
    "class Credentials(metaclass=ABCMeta):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "class SSMPSCredentials(Credentials):\n",
    "    def __init__(self, secretid: str):\n",
    "        self._secretid = secretid\n",
    "        self._secrets = {}\n",
    "        \n",
    "    def get_keys(self):\n",
    "        \"\"\"\n",
    "        credential fetching \n",
    "        \"\"\"\n",
    "        _aws_sm_args = {'service_name': 'secretsmanager', 'region_name': 'us-east-1'}\n",
    "        secrets_client = boto3.client(**_aws_sm_args)\n",
    "        get_secret_value_response = secrets_client.get_secret_value(SecretId=self._secretid)\n",
    "        return get_secret_value_response\n",
    "    \n",
    "    \n",
    "class BaseConnector(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def connect(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class SnowflakeConnector(BaseConnector):\n",
    "    def __init__(self, credentials: Credentials):\n",
    "        keys = credentials.get_keys()\n",
    "        self._secrets = json.loads(keys.get('SecretString', \"{}\"))\n",
    "\n",
    "    def connect(self, dbname: str, schema: str = 'DEFAULT'):\n",
    "        ctx = snowflake.connector.connect(\n",
    "            user=self._secrets['login_name'],\n",
    "            password=self._secrets['login_password'],\n",
    "            account=self._secrets['account'],\n",
    "            warehouse=self._secrets['warehouse'],\n",
    "            database=dbname,\n",
    "            schema=schema\n",
    "        )\n",
    "        return ctx\n",
    "\n",
    "    \n",
    "def run_query(querystr, ctx):\n",
    "    cursor_list = ctx.execute_string(\n",
    "        querystr\n",
    "        )\n",
    "    df = pd.DataFrame.from_records(cursor_list[-1].fetchall(), columns=[x[0] for x in cursor_list[-1].description])\n",
    "    df.columns= df.columns.str.lower()\n",
    "    return df\n",
    "\n",
    "import plotly.express as px\n",
    "def get_simple_plot(df_plt, x, y, grpby, text, title=''):\n",
    "    if title=='':\n",
    "        title = f'{y} vs {x}'\n",
    "    df_plt[grpby] = df_plt[grpby].astype(str)\n",
    "    fig = px.line(df_plt,\n",
    "                  x=x, \n",
    "                  y=y, \n",
    "                  title=title,\n",
    "                  color=grpby, \n",
    "                  hover_data=[text],\n",
    "                  width=800, height=400)\n",
    "    fig.show()\n",
    "    return \n",
    "\n",
    "\n",
    "## Credentials\n",
    "SF_CREDS = 'datascience-max-dev-sagemaker-notebooks'\n",
    "\n",
    "## Snowflake connection \n",
    "# conn=SnowflakeConnector(SSMPSCredentials(SF_CREDS))\n",
    "# ctx=conn.connect(\"MAX_PROD\",\"DATASCIENCE_STAGE\")\n",
    "# cur = ctx.cursor()\n",
    "\n",
    "input_bucket=\"hbo-ingest-datascience-content-dev\"\n",
    "output_bucket=\"hbo-outbound-datascience-content-dev\"\n",
    "key_path = 'cost_allocation/dev'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f78fff8-1ecb-497b-8beb-6b5760095e4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## SVOD Monthly, Retail,  domestic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d732a3-4567-4d75-8419-410162136776",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def exponential_decay(x, a, b,c):\n",
    "    return a * np.exp(b * x) + c\n",
    "\n",
    "def exponential_decay_slope(x, a, b):\n",
    "    return a * b*np.exp(b * x)\n",
    "\n",
    "def fit_exponential(x_data, y_data, p0, param_bounds):\n",
    "    x_fit = np.linspace(0, x_data.max(), 100)   \n",
    "    params, _ = curve_fit(exponential_decay, np.array(x_data), y_data, p0, bounds=param_bounds)\n",
    "    return x_fit, params\n",
    "\n",
    "\n",
    "def get_churn_bin(df_in, grpby):\n",
    "    df = df_in.groupby(by=['hbo_uuid','sub_month']+ grpby +['is_cancel']).sum().reset_index()\n",
    "    df = df_in[df_in.monthly_hours_viewed<=60]\n",
    "    nbins = 100\n",
    "    df['hours_viewed_bin'] = pd.qcut(df['monthly_hours_viewed'], np.linspace(0,1,nbins), duplicates='drop')\n",
    "    df['hours_viewed_bin'] = df['hours_viewed_bin'].apply(lambda x: (x.left+x.right)/2)\n",
    "    df['hours_viewed_bin'] = df['hours_viewed_bin'].astype('float')\n",
    "    df['churn'] = 1*df['is_cancel']  \n",
    "    \n",
    "    df_bin = df.groupby(['hours_viewed_bin']+grpby).agg({'churn':'mean', 'hbo_uuid':'count',\n",
    "                                                         'is_cancel':'sum','monthly_hours_viewed':'sum'}).reset_index()\n",
    "    return(df_bin)\n",
    "\n",
    "\n",
    "\n",
    "def get_df_60_h(list_df):\n",
    "    df_list=[]\n",
    "    num=0\n",
    "    for df_test in list_df:\n",
    "        df_test['num_df'] = num\n",
    "        df_list.append(df_test)\n",
    "        num=num+1\n",
    "    return(df_list)\n",
    "\n",
    "\n",
    "def get_simple_plot_multiple(df_plt, x, y, x_fit, y_fit, params, title=''):\n",
    "    if title=='':\n",
    "        \n",
    "        title = f'{y} vs {x}'\n",
    "       \n",
    "    a_fit, b_fit, c_fit = params\n",
    "    annotation_x_loc = 50\n",
    "    annotation_y_loc = y_fit.min() +(y_fit.max()  - y_fit.min() )/2 \n",
    "        \n",
    "    fig = px.scatter(df_plt,\n",
    "                  x=x, \n",
    "                  y=y, \n",
    "                  title=title,\n",
    "                  width=500, height=400)\n",
    "    fig.add_scatter( \n",
    "              x=x_fit, \n",
    "              y=y_fit)\n",
    "\n",
    "    fig.update_layout(\n",
    "        template='simple_white',\n",
    "        showlegend=False,\n",
    "        xaxis=dict(range=[0,50]),\n",
    "        annotations=[\n",
    "        dict(\n",
    "            x=annotation_x_loc,  # x-coordinate for the text\n",
    "            y=annotation_y_loc,  # y-coordinate for the text\n",
    "            text='y= {:.2f} * e^({:.2f} * hours_viewed) + {:.2f}'.format(a_fit, b_fit, c_fit),  # the text to display\n",
    "            showarrow=False,  # disable arrow for the annotation\n",
    "            xanchor='right',\n",
    "            font=dict(\n",
    "                family='Arial',  # specify font family\n",
    "                size=18,  # specify font size\n",
    "                color='black'  # specify font color\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ") \n",
    "    fig.show()\n",
    "    return \n",
    "\n",
    "def get_simple_plot_multiple_dot(df_plt, x, y, x_fit, y_fit, params, x_med, y_med, title=''):\n",
    "    if title=='':\n",
    "        \n",
    "        title = f'{y} vs {x}'\n",
    "       \n",
    "    a_fit, b_fit, c_fit = params\n",
    "    print('y= {:.2f} * e^({:.2f} * hours_viewed) + {:.2f}'.format(a_fit, b_fit, c_fit))\n",
    "    print('y= {:.3f} * e^({:.2f} * hours_viewed)'.format(a_fit*b_fit,b_fit))\n",
    "    annotation_x_loc = 50\n",
    "    annotation_y_loc = y_fit.min() +(y_fit.max()  - y_fit.min() )/2 \n",
    "        \n",
    "    fig = px.scatter(df_plt,\n",
    "                  x=x, \n",
    "                  y=y, \n",
    "                  title=title,\n",
    "                  width=500, height=400)\n",
    "    fig.add_scatter( \n",
    "              x=x_fit, \n",
    "              y=y_fit)\n",
    "    \n",
    "    fig.add_scatter( \n",
    "              x=x_med, \n",
    "              y=y_med,\n",
    "                mode='markers',\n",
    "            marker=dict(size=14, color='red', line=dict(color='black', width=2)))\n",
    "\n",
    "    fig.update_layout(\n",
    "        template='simple_white',\n",
    "        showlegend=False,\n",
    "        xaxis=dict(range=[0,50]),\n",
    "        annotations=[\n",
    "        dict(\n",
    "            x=x_med+0.2,  # x-coordinate for the text\n",
    "            y=y_med+0.01,  # y-coordinate for the text\n",
    "            text='{:.2f}, {:.2f}'.format(x_med, y_med),  # the text to display\n",
    "            showarrow=False,  # disable arrow for the annotation\n",
    "            xanchor='left',\n",
    "            font=dict(\n",
    "                family='Arial',  # specify font family\n",
    "                size=18,  # specify font size\n",
    "                color='black'  # specify font color\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ") \n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "def get_churn_plot_simple(df_i, title, param_dic, x_med=0):\n",
    "    df_i = df_i[df_i.is_cancel>=20]\n",
    "#         display(df_i.tail(5))\n",
    "\n",
    "    x_var = df_i.hours_viewed_bin\n",
    "    y_data = df_i.churn\n",
    "    p0 = [0.5, -0.1, 0.01] \n",
    "    param_bounds = ([0, -0.8, 0.01], [np.inf, -0.1, np.inf])\n",
    "\n",
    "    x_fit, params = fit_exponential(x_var, y_data, p0, param_bounds)\n",
    "    a_fit, b_fit, c_fit = params\n",
    "    y_fit = exponential_decay(x_fit, a_fit, b_fit, c_fit)\n",
    "    \n",
    "    if x_med==0:\n",
    "        fig = get_simple_plot_multiple(df_i, 'hours_viewed_bin', 'churn', x_fit, y_fit, params, f'{title}')\n",
    "    else:\n",
    "        y_med = exponential_decay(x_med, a_fit, b_fit, c_fit)\n",
    "        print(x_med)\n",
    "        print(y_med)\n",
    "        fig = get_simple_plot_multiple_dot(df_i, 'hours_viewed_bin', 'churn', x_fit, y_fit, params, x_med, np.array(y_med), f'{title}')\n",
    "    display(df_i.head())\n",
    "    param_dic['acquired'] = params\n",
    "    return fig, params\n",
    "\n",
    "\n",
    "\n",
    "def get_simple_plot_dot(df_plt, x, y, x_fit, y_fit, params, x_med, y_med, title=''):\n",
    "    if title=='':\n",
    "        \n",
    "        title = f'{y} vs {x}'\n",
    "       \n",
    "    a_fit, b_fit, c_fit = params\n",
    "    print('y= {:.2f} * e^({:.2f} * hours_viewed) + {:.2f}'.format(a_fit, b_fit, c_fit))\n",
    "    print('y= {:.3f} * e^({:.2f} * hours_viewed)'.format(a_fit*b_fit,b_fit))\n",
    "    annotation_x_loc = 50\n",
    "    annotation_y_loc = y_fit.min() +(y_fit.max()  - y_fit.min() )/2 \n",
    "        \n",
    "    fig = px.line(x=x_fit, \n",
    "                  y=y_fit, \n",
    "                  title=title,\n",
    "                  width=500, height=400)\n",
    "    fig.add_scatter( \n",
    "              x=x_med, \n",
    "              y=y_med,\n",
    "                mode='markers',\n",
    "            marker=dict(size=14, color='red', line=dict(color='black', width=2)))\n",
    "\n",
    "    fig.update_layout(\n",
    "        template='simple_white',\n",
    "        showlegend=False,\n",
    "        xaxis=dict(range=[0,50]),\n",
    "        xaxis_title= \"hours_viewed_bin\",\n",
    "        yaxis_title= \"Change in churn rate (slope)\",\n",
    "        annotations=[\n",
    "        dict(\n",
    "            x=x_med+0.25,  # x-coordinate for the text\n",
    "            y=y_med+0.0005,  # y-coordinate for the text\n",
    "            text='{:.2f}, {:.4f}'.format(x_med, y_med),  # the text to display\n",
    "            showarrow=False,  # disable arrow for the annotation\n",
    "            xanchor='left',\n",
    "            font=dict(\n",
    "                family='Arial',  # specify font family\n",
    "                size=18,  # specify font size\n",
    "                color='black'  # specify font color\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ") \n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "def get_churn_slope_plot_simple(df_i, title, params, x_med=0):\n",
    "    df_i = df_i[df_i.is_cancel>=20]\n",
    "#         display(df_i.tail(5))\n",
    "\n",
    "    x_var = df_i.hours_viewed_bin\n",
    "    x_fit = np.linspace(0, x_var.max(), 100)   \n",
    "    a_fit, b_fit, c_fit = params\n",
    "    y_fit = exponential_decay_slope(x_fit, a_fit, b_fit)\n",
    "    \n",
    "    y_med = exponential_decay_slope(x_med, a_fit, b_fit)\n",
    "    print(x_med)\n",
    "    print(y_med)\n",
    "    fig = get_simple_plot_dot(df_i, 'hours_viewed_bin', 'churn', x_fit, y_fit, params, x_med, np.array(y_med), f'{title}')\n",
    "    display(df_i.head())\n",
    "    param_dic['acquired'] = params\n",
    "    return fig\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "badf6cd6-04cf-4f2a-a0f8-50967b139d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## EMEA markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8d28d7d-26fb-4e49-9c05-db2adf6e293d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bc78ca7-ca5f-4245-a84f-29754002ab56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_60_00 = Utils.read_csv_s3(input_bucket, key_path, 'churn_user_stream_full_latam_60d_20230101_full.csv')\n",
    "df_60_0 = Utils.read_csv_s3(input_bucket, key_path, 'churn_user_stream_full_latam_60d_20230301_full.csv')\n",
    "df_60_1 = Utils.read_csv_s3(input_bucket, key_path, 'churn_user_stream_full_latam_60d_20230501_full.csv')\n",
    "df_60_2 = Utils.read_csv_s3(input_bucket, key_path, 'churn_user_stream_full_latam_60d_20230701_full.csv')\n",
    "\n",
    "def get_df_test(df_test):\n",
    "    df_test['tenure_months'] = df_test['sub_month']\n",
    "    df_test['monthly_hours_viewed'] = np.where(df_test['tenure_months']>1, df_test['hours_viewed']/2, df_test['hours_viewed'])\n",
    "    user_total = df_test.groupby(['hbo_uuid'])['monthly_hours_viewed'].transform('sum')\n",
    "    df_test['frc'] = df_test['monthly_hours_viewed'] / user_total\n",
    "    \n",
    "    df_test['program_type'] = np.where((df_test.program_type=='original') & (df_test.old_new=='library'), 'acquired', df_test.program_type)\n",
    "    df_test = df_test[df_test.tenure_months>2]\n",
    "    df_test = df_test.fillna(0)\n",
    "    return(df_test)\n",
    "\n",
    "df_60_00=get_df_test(df_60_00)\n",
    "df_60_0=get_df_test(df_60_0)\n",
    "df_60_1=get_df_test(df_60_1)\n",
    "df_60_2=get_df_test(df_60_2)\n",
    "\n",
    "df_list = get_df_60_h([df_60_00, df_60_0, df_60_1, df_60_2])\n",
    "df_60 = pd.concat(df_list)\n",
    "Utils.to_pkl_s3(df_60,input_bucket, key_path, 'df_latam_o_2023.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e5cc1d2-fd04-4b1e-b149-0506e9183669",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_60_pr[['frc']][df_60_pr.program_type=='acquired'].describe(percentiles=[0.1,0.2,0.25,0.3,0.35,0.4,0.5,0.55,0.8,0.9,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6354f902-ea81-4503-a9e7-9b1c8a4ee03c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ## Nordic   \n",
    "\n",
    "df_60 = Utils.read_pkl_s3(input_bucket, key_path, 'df_latam_o_2023.pkl')\n",
    "\n",
    "## Hours viewed \n",
    "print('hours viewed distribution')\n",
    "df_60_user = df_60.groupby(by=['hbo_uuid','is_cancel','num_df','sub_month']).sum().reset_index()\n",
    "display(df_60_user.describe())\n",
    "# get_histogram(df_60_user[df_60_user.monthly_hours_viewed<=50].sample(n=1000), 'monthly_hours_viewed', f'Hours_viewed audience distribution,total')\n",
    "\n",
    "## hours viewed breakdown\n",
    "print('hours viewed breakdown median')\n",
    "df_60_pr = df_60.groupby(by=['hbo_uuid','sub_month','num_df','program_type']).sum().reset_index()\n",
    "display(df_60_pr.groupby(by=['program_type']).median())\n",
    "display(df_60_pr[['frc']][df_60_pr.program_type=='acquired'].describe(percentiles=[0.1,0.2,0.25,0.3,0.4,0.5,0.75,1]))\n",
    "\n",
    "target_frc_list = [0.99, 0.9, 0.8, 0.7]\n",
    "nor = []\n",
    "for target_frc in target_frc_list:\n",
    "    percentile = percentileofscore(df_60_pr[df_60_pr.program_type=='acquired']['frc'], target_frc)\n",
    "    nor.append({'acquired_hr_fraction': target_frc, 'percentile': percentile})\n",
    "\n",
    "## Average churn\n",
    "print(df_60_user.shape, df_60_user.is_cancel.sum()/df_60_user.shape[0])\n",
    "\n",
    "## Tenure churn\n",
    "print('tenure churn')\n",
    "df_60_month = df_60.groupby(by=['sub_month']).agg({'hbo_uuid':'count','is_cancel':'sum'})\n",
    "df_60_month['churn'] = df_60_month['is_cancel'] /df_60_month['hbo_uuid'] \n",
    "display(df_60_month)\n",
    "\n",
    "## some months have high churns (e.g. 17) investigate later \n",
    "df_60 = df_60[df_60.sub_month<=24]\n",
    " \n",
    "\n",
    "## Total \n",
    "param_dic = {}\n",
    "df_60_t = df_60.groupby(by=['hbo_uuid','is_cancel','sub_month']).sum().reset_index()\n",
    "df_60_s = get_churn_bin(df_60_t, [])\n",
    "param_dic = get_churn_plot_simple(df_60_s, 'total', param_dic)\n",
    "\n",
    "## Acquired \n",
    "param_dic = {}\n",
    "df_60_p= df_60.groupby(by=['hbo_uuid','is_cancel','sub_month','num_df','program_type']).sum().reset_index()\n",
    "\n",
    "## Get median \n",
    "df_med= df_60_user[['hbo_uuid','sub_month','num_df']].merge(df_60_p[df_60_p.program_type=='acquired'], on=['hbo_uuid','sub_month','num_df'], how='left')\n",
    "df_med = df_med.fillna(0)\n",
    "med_x = df_med.monthly_hours_viewed.median()\n",
    "\n",
    "df_60_s = df_60_p[df_60_p.frc>=0.9]\n",
    "df_60_s = get_churn_bin(df_60_s, ['program_type'])\n",
    "fig_nor, params = get_churn_plot_simple(df_60_s[df_60_s.program_type=='acquired'], 'LATAM acquired', param_dic, np.array(med_x))\n",
    "fig_nor_slope = get_churn_slope_plot_simple(df_60_s[df_60_s.program_type=='acquired'], 'LATAM acquired slope', params, np.array(med_x))\n",
    "\n",
    "df_nor = df_60_s[df_60_s.program_type=='acquired']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd17bbdc-4613-4d87-954a-c6ddc4e1f854",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_nor[['hours_viewed_bin','churn','hbo_uuid','is_cancel']]\n",
    "df = df.rename(columns={'hbo_uuid':'uuid_count', 'is_cancel':'cancel_count'})\n",
    "df.to_csv('latam_market_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16c343b1-295e-42a0-82e0-383e8c9f8185",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d289e1a4-290d-4bb5-b65a-226673a6bfc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "709c9818-4f96-4311-bd9a-6a04c18a9440",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "eda_churn_2311_latam_genpop_ce",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

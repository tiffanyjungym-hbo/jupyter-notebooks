{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb9a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### nodejs:  https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-up-node-on-ec2-instance.html\n",
    "\n",
    "# !pip install \"jupyterlab>=3\" \"ipywidgets>=7.6\"\n",
    "# !pip install jupyter-dash\n",
    "# !jupyter lab build\n",
    "\n",
    "# !pip install snowflake --user\n",
    "# !pip install snowflake-connector-python --userqr4\n",
    "# !pip install category_encoders\n",
    "# !pip install xgboost\n",
    "# !pip install fuzzywuzzy --user\n",
    "# !pip install lightgbm --user\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "path=!pwd\n",
    "sys.path.append(os.path.join(path[0], '..'))\n",
    "sys.path.append('/home/ec2-user/SageMaker/jupyter-notebooks/')\n",
    "from utils import *\n",
    "import snowflake.connector\n",
    "from datetime import timedelta\n",
    "\n",
    "from category_encoders import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import r2_score as r2_score\n",
    "import sklearn.model_selection\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23352108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnowflakeConnector(BaseConnector):\n",
    "    def __init__(self, credentials: Credentials):\n",
    "        keys = credentials.get_keys()\n",
    "        self._secrets = json.loads(keys.get('SecretString', \"{}\"))\n",
    "\n",
    "    def connect(self, dbname: str, schema: str = 'DEFAULT'):\n",
    "        ctx = snowflake.connector.connect(\n",
    "            user=self._secrets['login_name'],\n",
    "            password=self._secrets['login_password'],\n",
    "            account=self._secrets['account'],\n",
    "            warehouse=self._secrets['warehouse'],\n",
    "            database=dbname,\n",
    "            schema=schema\n",
    "        )\n",
    "\n",
    "        return ctx\n",
    "\n",
    "def run_query(querystr, ctx):\n",
    "    cursor_list = ctx.execute_string(\n",
    "        querystr\n",
    "        )\n",
    "    df = pd.DataFrame.from_records(cursor_list[-1].fetchall(), columns=[x[0] for x in cursor_list[-1].description])\n",
    "    df.columns= df.columns.str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "## Credentials\n",
    "SF_CREDS = 'datascience-max-dev-sagemaker-notebooks'\n",
    "\n",
    "## Snowflake connection \n",
    "conn=SnowflakeConnector(SSMPSCredentials(SF_CREDS))\n",
    "ctx=conn.connect(\"MAX_PROD\",\"DATASCIENCE_STAGE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70883ecd",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811c305c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220113 saved\n"
     ]
    }
   ],
   "source": [
    "for val_date in ['20201201']:\n",
    "    val_date_file = datetime.strptime(val_date, '%Y%m%d').strftime('%Y-%m-%d')\n",
    "\n",
    "    ### Base assets \n",
    "    querystr_base_assets = '''\n",
    "    --Step 4: Gather past metrics and create the basic heuristic forecast, plus median metrics tables\n",
    "    create or replace table max_dev.workspace.psi_past_base_assets as \n",
    "    (select distinct\n",
    "          a.title_id\n",
    "        , coalesce(a.season_number,0) as season_number\n",
    "        , a.viewable_id\n",
    "        , title_name\n",
    "        , first_offered_date::date as asset_max_premiere\n",
    "        , end_utc_max::date as asset_max_end_dt\n",
    "        , coalesce(raod.season_first_offered_date::date,raod.title_first_offered_date::date) as season_premiere\n",
    "        , asset_run_time\n",
    "        , a.content_category\n",
    "        , episode_number_in_season\n",
    "        , content_source\n",
    "        , program_type\n",
    "        , category\n",
    "        , tier\n",
    "        , viewership_start_date as effective_start_date\n",
    "        , viewership_end_date as effective_end_date\n",
    "    from max_prod.catalog.reporting_asset_dim a\n",
    "    join max_prod.catalog.reporting_asset_offering_dim raod\n",
    "        on a.viewable_id = raod.viewable_id\n",
    "        and brand = 'HBO MAX'\n",
    "        and territory = 'HBO MAX DOMESTIC'\n",
    "        and channel = 'HBO MAX SUBSCRIPTION'\n",
    "    inner join max_prod.content_analytics.psi_past_title_metadata b\n",
    "        on a.title_id = b.viewership_title_id\n",
    "        and coalesce(a.season_number,0) = coalesce(b.viewership_season_number,0)\n",
    "    where 1 = 1\n",
    "    and asset_type IN ('FEATURE','ELEMENT')\n",
    "    and start_utc_max is not null\n",
    "    and a.content_category in ('movies','series','special')\n",
    "    and coalesce(raod.season_first_offered_date,raod.title_first_offered_date)  >= '2020-05-27 07:01:00.000'\n",
    "    order by season_premiere, title_name \n",
    "    );\n",
    "    select * from max_dev.workspace.psi_past_base_assets;\n",
    "    '''\n",
    "\n",
    "\n",
    "    ### Train fv \n",
    "    querystr_train_fv=''' \n",
    "    set val_date = date({val_date}, 'YYYYMMDD');\n",
    "    --Step 4: Gather past metrics and create the basic heuristic forecast, plus median metrics tables\n",
    "    create or replace table max_dev.workspace.psi_past_base_full as (\n",
    "    with fv as (\n",
    "        select\n",
    "              b.title_id\n",
    "            , b.title_name\n",
    "            , b.season_number\n",
    "            , b.content_category\n",
    "            , b.category\n",
    "            , tier\n",
    "            , request_time_gmt::date as request_date\n",
    "            , count(distinct concat(hbo_uuid, subscription_id)) as first_views\n",
    "        from MAX_PROD.BI_ANALYTICS.SUBSCRIPTION_FIRST_CONTENT_WATCHED a\n",
    "        inner join max_dev.workspace.psi_past_base_assets b\n",
    "            on a.viewable_id = b.viewable_id\n",
    "            --and request_time_gmt::date between season_premiere_date and dateadd('day',90,season_premiere_date)\n",
    "            --and season_premiere_date >= '2020-05-27 07:00:01'\n",
    "        where 1 = 1\n",
    "            and request_time_gmt::date between asset_max_premiere and asset_max_end_dt\n",
    "            and request_time_gmt::date between effective_start_date and effective_end_date\n",
    "            and request_time_gmt::date < dateadd('days',-1,$val_date)\n",
    "            and country_iso_code in ('US','PR','GU')\n",
    "        group by 1,2,3,4,5,6,7\n",
    "        --order by 2,4\n",
    "    )\n",
    "    , hv as (\n",
    "        select\n",
    "              b.title_id\n",
    "            , b.title_name\n",
    "            , b.season_number\n",
    "            , b.content_category\n",
    "            , b.category\n",
    "            , tier\n",
    "            , request_time_gmt::date as request_date\n",
    "            , coalesce(round(sum(stream_elapsed_play_seconds)/3600,3), 0) as hours_viewed\n",
    "        from max_prod.viewership.max_user_stream_heartbeat a\n",
    "        inner join max_dev.workspace.psi_past_base_assets b\n",
    "            on a.viewable_id = b.viewable_id\n",
    "        where 1 = 1\n",
    "        and stream_elapsed_play_seconds >= 120\n",
    "        and request_time_gmt > '2020-05-27 07:00:00'\n",
    "        and request_time_gmt::date between asset_max_premiere and asset_max_end_dt\n",
    "        and request_time_gmt::date between effective_start_date and effective_end_date\n",
    "        and request_time_gmt::date < dateadd('days',-1,$val_date)\n",
    "        group by 1,2,3,4,5,6,7\n",
    "    )\n",
    "    , dates as (\n",
    "        select distinct\n",
    "              rs.title_id\n",
    "            , rs.title_name\n",
    "            , rs.season_number\n",
    "            , rs.content_category\n",
    "            , rs.content_source\n",
    "            , rs.program_type\n",
    "            , rs.category\n",
    "            , rs.tier\n",
    "            --, rs.season_premiere\n",
    "            , rs.effective_start_date\n",
    "            , request_date\n",
    "            , case when request_date::date = effective_start_date::date then 1 else 0 end as premiere_ind\n",
    "            , count(distinct case when request_date::date = asset_max_premiere::date then viewable_id else null end) as asset_premiere_count\n",
    "            , round(sum(distinct case when request_date::date = asset_max_premiere::date then asset_run_time else 0 end)/3600,3) as premiering_hours_runtime\n",
    "        from max_dev.workspace.psi_past_base_assets rs\n",
    "        cross join (\n",
    "            select distinct seq_date as request_date \n",
    "            from max_prod.staging.date_range \n",
    "            where seq_date < '2024-12-31'::date\n",
    "        ) rd\n",
    "        where rd.request_date between \n",
    "        coalesce(rs.effective_start_date,rs.season_premiere,rs.asset_max_premiere) \n",
    "        and dateadd('days',90,coalesce(rs.effective_start_date,rs.season_premiere,rs.asset_max_premiere))\n",
    "          and rd.request_date between effective_start_date and effective_end_date\n",
    "        group by 1,2,3,4,5,6,7,8,9,10,11\n",
    "        order by 2,3,8\n",
    "    )\n",
    "        select dt.*\n",
    "            , coalesce(first_views,0) as first_views\n",
    "            , coalesce(hours_viewed,0) as hours_viewed\n",
    "            , dt.request_date - effective_start_date as days_since_premiere\n",
    "            , $val_date - effective_start_date -1 as days_on_platform\n",
    "            , case when $val_date - effective_start_date - 1 >=\n",
    "                case when dt.category = 'Popcorn' and year(effective_start_date) < 2022 then 31 else 90 end\n",
    "            then 1 else 0 end as finished_window_flag\n",
    "        from dates dt\n",
    "        left join hv\n",
    "            on dt.title_id = hv.title_id\n",
    "            and dt.season_number = hv.season_number\n",
    "            and dt.request_date = hv.request_date\n",
    "            and dt.content_category = hv.content_category\n",
    "            and dt.category = hv.category\n",
    "            and dt.tier = hv.tier\n",
    "        left join fv\n",
    "            on dt.title_id = fv.title_id\n",
    "            and dt.season_number = fv.season_number\n",
    "            and dt.request_date = fv.request_date\n",
    "            and dt.content_category = fv.content_category\n",
    "            and dt.category = fv.category\n",
    "            and dt.tier = fv.tier\n",
    "        where 1 = 1\n",
    "        --and dt.title_name like 'In Treatment'\n",
    "        order by title_id, title_name, season_number, category, request_date\n",
    "    );\n",
    "    select\n",
    "    * \n",
    "    from max_dev.workspace.psi_past_base_full\n",
    "    '''.format(val_date = val_date)\n",
    "\n",
    "\n",
    "    ## Train imdb \n",
    "    querystr_train_imdb='''\n",
    "    select distinct\n",
    "          pba.title_id\n",
    "        , coalesce(pba.season_number,0) as season_number\n",
    "        , pba.viewable_id\n",
    "        , pba.title_name\n",
    "        , pba.content_category\n",
    "        , pba.program_type\n",
    "        , pba.category\n",
    "        , pba.tier\n",
    "        , pba.effective_start_date\n",
    "        , pba.effective_end_date\n",
    "        , coalesce(ivm.imdb_id, ivm.imdb_series_id) as imdb_imdb_series_id\n",
    "        , imc.reference_type\n",
    "        , itr.original_title as reference_title\n",
    "        , itr.title_id as reference_title_id\n",
    "        , itr.title_type as reference_title_type\n",
    "        , imcr.reference_type as reference_reference_type\n",
    "        , itrr.title_id as reference_reference_title_id\n",
    "    from max_dev.workspace.psi_past_base_assets pba\n",
    "    left join max_prod.editorial.imdb_viewable_map ivm\n",
    "        on pba.title_id = coalesce(ivm.viewable_id, ivm.viewable_series_id) \n",
    "    left join enterprise_data.catalog.imdb_title it \n",
    "        on coalesce(ivm.imdb_id, ivm.imdb_series_id) = it.title_id\n",
    "    left join enterprise_data.catalog.imdb_movie_connection imc \n",
    "        on it.title_id = imc.title_id\n",
    "    left join enterprise_data.catalog.imdb_title itr \n",
    "        on itr.title_id = imc.reference_title_id\n",
    "    left join enterprise_data.catalog.imdb_movie_connection imcr\n",
    "        on itr.title_id = imcr.title_id\n",
    "        and imcr.reference_type in ('featured_in')\n",
    "    left join enterprise_data.catalog.imdb_title itrr \n",
    "        on itrr.title_id = imcr.reference_title_id\n",
    "    where 1 = 1\n",
    "      and imc.reference_type in ('follows','spin_off_from','remake_of','version_of','featured_in')\n",
    "    order by effective_start_date, title_name\n",
    "    ;\n",
    "    '''\n",
    "\n",
    "\n",
    "    ### future predict data pull \n",
    "    querystr_pred_fv='''\n",
    "    select distinct\n",
    "        fp.title as title_name\n",
    "        , ft.imdb_title_id as imdb_imdb_series_id\n",
    "        , fp.season as season_number\n",
    "        , fp.tier\n",
    "        , fp.category\n",
    "        , ft.content_category\n",
    "        , fp.premiere_date as effective_start_date\n",
    "        , fp.schedule_label\n",
    "        , it.original_title as imdb_title_name\n",
    "        , it.number_of_votes as n_votes\n",
    "        , imc.reference_type\n",
    "        , itr.original_title as reference_title\n",
    "        , itr.title_id as reference_title_id\n",
    "        , itr.title_type as reference_title_type\n",
    "        , itr.number_of_votes as reference_n_votes\n",
    "        , imcr.reference_type as reference_reference_type\n",
    "        , itrr.title_id as reference_reference_title_id\n",
    "    from max_prod.content_analytics.daily_future_programming_schedule fp\n",
    "    left join max_dev.workspace.future_title_imdb_map ft\n",
    "        on fp.title = ft.title_name\n",
    "    left join enterprise_data.catalog.imdb_title it \n",
    "        on ft.imdb_title_id = it.title_id\n",
    "    left join enterprise_data.catalog.imdb_movie_connection imc \n",
    "        on it.title_id = imc.title_id\n",
    "        and imc.reference_type in ('follows','spin_off_from','remake_of','version_of','featured_in')\n",
    "    left join enterprise_data.catalog.imdb_title itr \n",
    "        on itr.title_id = imc.reference_title_id\n",
    "    left join enterprise_data.catalog.imdb_movie_connection imcr\n",
    "        on itr.title_id = imcr.title_id\n",
    "        and imcr.reference_type in ('featured_in')\n",
    "    left join enterprise_data.catalog.imdb_title itrr \n",
    "        on itrr.title_id = imcr.reference_title_id\n",
    "    order by effective_start_date, title_name\n",
    "    ;\n",
    "    '''\n",
    "\n",
    "#     df = run_query(querystr_base_assets, ctx)\n",
    "#     df_train_fv = run_query(querystr_train_fv, ctx)\n",
    "#     df_train_fv.to_csv('s3://hbo-ingest-datascience-content-dev/psi_first_views/fv_train_{}.csv'.format(val_date_file))\n",
    "#     df_train_imdb = run_query(querystr_train_imdb, ctx)\n",
    "#     df_train_imdb.to_csv('s3://hbo-ingest-datascience-content-dev/psi_first_views/fv_train_imdb_{}.csv'.format(val_date_file))\n",
    "\n",
    "\n",
    "#     ### Predict future \n",
    "    df_pred = run_query(querystr_pred_fv, ctx)\n",
    "    df_pred['title_id'] = 0\n",
    "    df_pred.to_csv('s3://hbo-ingest-datascience-content-dev/psi_first_views/fv_pred_{}.csv'.format(val_date_file))\n",
    "\n",
    "    print(f'{val_date} saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad930a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_fv.to_csv('s3://hbo-ingest-datascience-content-dev/psi_firstviews/fv_pred_bt_{}.csv'.format(val_date_file))\n",
    "# df_train_imdb.to_csv('s3://hbo-ingest-datascience-content-dev/psi_firstviews/fv_pred_imdb_bt_{}.csv'.format(val_date_file))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0afab9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 2.7.4, Python Version: 3.6.13, Platform: Linux-4.14.252-131.483.amzn1.x86_64-x86_64-with-glibc2.9\n",
      "INFO:snowflake.connector.connection:This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.\n",
      "INFO:root:Reading from hbo-outbound-datascience-content-dev/psi_first_views/dev/fv_pred_munged_2022-06-23_adhoc.csv\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDenied) when calling the GetObject operation: Access Denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9864475a8b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0mkey_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'psi_first_views/dev'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m \u001b[0mUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_bucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'fv_pred_munged_2022-06-23_adhoc.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0mdf_pred_future_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_pred_future_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0mcvdf_to_snowflake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pred_future_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'firstview_postgl_temp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9864475a8b7c>\u001b[0m in \u001b[0;36mread_csv_s3\u001b[0;34m(bucket, key_path, filename)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Reading from {bucket}/{key}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\\\N\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    414\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDenied) when calling the GetObject operation: Access Denied"
     ]
    }
   ],
   "source": [
    "##nodejs:  https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-up-node-on-ec2-instance.html\n",
    "\n",
    "# !pip install \"jupyterlab>=3\" \"ipywidgets>=7.6\"\n",
    "# !pip install jupyter-dash\n",
    "# !jupyter lab build\n",
    "\n",
    "\n",
    "# !pip install snowflake --user\n",
    "# !pip install snowflake-connector-python --user\n",
    "# !pip install category_encoders\n",
    "# !pip install xgboost\n",
    "# !pip install lightgbm --user\n",
    "import os\n",
    "import sys\n",
    "path=!pwd\n",
    "# sys.path.append(os.path.join(path[0], '..'))\n",
    "# sys.path.append('/home/ec2-user/SageMaker/jupyter-notebooks/')\n",
    "# from utils import *\n",
    "import snowflake.connector\n",
    "from datetime import timedelta\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "import logging \n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from category_encoders import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import r2_score as r2_score\n",
    "import sklearn.model_selection\n",
    "\n",
    "from io import StringIO\n",
    "class Utils():\n",
    "    @staticmethod\n",
    "    def to_csv_s3(content, bucket, key_path, filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        csv_buffer = StringIO()\n",
    "        content.to_csv(csv_buffer)\n",
    "        client.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n",
    "        logger.info(f'Saved to {bucket}/{key}')\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_pkl_s3(content, bucket, key_path, filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        obj = pickle.dumps(content)\n",
    "        client.put_object(Bucket=bucket, Key=key, Body=obj)\n",
    "        logger.info(f'Saved model to {os.path.join(bucket, key)}')\n",
    "        logger.info(f'Saved to {bucket}/{key}')\n",
    "\n",
    "    @staticmethod\n",
    "    def read_csv_s3(bucket, key_path,filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        logger.info(f'Reading from {bucket}/{key}')\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        df = pd.read_csv(obj['Body'], na_values=\"\\\\N\")\n",
    "        return df\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_pkl_s3(bucket, key_path,filename):\n",
    "        client = boto3.client('s3')\n",
    "        key = os.path.join(key_path, filename)\n",
    "        logger.info(f'Reading from {bucket}/{key}')\n",
    "        obj = client.get_object(Bucket=bucket, Key=key)\n",
    "        body = obj['Body'].read()\n",
    "        model = pickle.loads(body)\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "class Credentials(metaclass=ABCMeta):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "class SSMPSCredentials(Credentials):\n",
    "    def __init__(self, secretid: str):\n",
    "        self._secretid = secretid\n",
    "        self._secrets = {}\n",
    "        \n",
    "    def get_keys(self):\n",
    "        \"\"\"\n",
    "        credential fetching \n",
    "        \"\"\"\n",
    "        _aws_sm_args = {'service_name': 'secretsmanager', 'region_name': 'us-east-1'}\n",
    "        secrets_client = boto3.client(**_aws_sm_args)\n",
    "        get_secret_value_response = secrets_client.get_secret_value(SecretId=self._secretid)\n",
    "        return get_secret_value_response\n",
    "    \n",
    "    \n",
    "class BaseConnector(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def connect(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class SnowflakeConnector(BaseConnector):\n",
    "    def __init__(self, credentials: Credentials):\n",
    "        keys = credentials.get_keys()\n",
    "        self._secrets = json.loads(keys.get('SecretString', \"{}\"))\n",
    "\n",
    "    def connect(self, dbname: str, schema: str = 'DEFAULT'):\n",
    "        ctx = snowflake.connector.connect(\n",
    "            user=self._secrets['login_name'],\n",
    "            password=self._secrets['login_password'],\n",
    "            account=self._secrets['account'],\n",
    "            warehouse=self._secrets['warehouse'],\n",
    "            database=dbname,\n",
    "            schema=schema\n",
    "        )\n",
    "\n",
    "        return ctx\n",
    "\n",
    "\n",
    "def run_query(query, dbname, schema):\n",
    "    SF_CREDS = 'datascience-max-dev-sagemaker-notebooks'\n",
    "\n",
    "    conn=SnowflakeConnector(SSMPSCredentials(SF_CREDS))\n",
    "    ctx=conn.connect(dbname,schema)\n",
    "    cursor = ctx.cursor()\n",
    "    cursor.execute(query)\n",
    "    df = pd.DataFrame(cursor.fetchall(), columns = [desc[0] for desc in cursor.description])\n",
    "    df.columns= df.columns.str.lower()\n",
    "    return df    \n",
    "\n",
    "## Credentials\n",
    "SF_CREDS = 'datascience-max-dev-sagemaker-notebooks'\n",
    "\n",
    "## Snowflake connection \n",
    "conn=SnowflakeConnector(SSMPSCredentials(SF_CREDS))\n",
    "ctx=conn.connect(\"MAX_PROD\",\"DATASCIENCE_STAGE\")\n",
    "cur = ctx.cursor()\n",
    "\n",
    "def cvdf_to_snowflake(df, table_name):\n",
    "    stage = '@HBO_OUTBOUND_DATASCIENCE_CONTENT_DEV'\n",
    "    output_bucket = \"hbo-outbound-datascience-content-dev\"\n",
    "    dbname, schema = 'MAX_DEV', 'WORKSPACE'\n",
    "    \n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index = False)\n",
    "    content = csv_buffer.getvalue()\n",
    "    client = boto3.client('s3')\n",
    "    client.put_object(Bucket=output_bucket, Key=table_name, Body=content)\n",
    "\n",
    "    print ('Create Table: ' + table_name)\n",
    " \n",
    "    run_query('''\n",
    "    create or replace table {table_name} (\n",
    "    title_name varchar,\n",
    "    effective_start_date varchar,\n",
    "    season_number int, \n",
    "    tier varchar,\n",
    "    content_category  varchar,\n",
    "    category varchar,\n",
    "    prequel_count int,\n",
    "    prequel_featured_count int,\n",
    "    prequel_featured_count_s int,\n",
    "    page_views int,\n",
    "    page_views_s int,\n",
    "    tier_adjusted int,\n",
    "    first_views_pred float,\n",
    "    model_pred_date varchar,\n",
    "    schedule_label varchar\n",
    "    )\n",
    "    '''.format(table_name = table_name), dbname, schema)\n",
    "\n",
    "    print ('Begin Uploading')\n",
    "    run_query('''\n",
    "    insert into max_dev.workspace.{table_name}\n",
    "\n",
    "    select \n",
    "              $1\n",
    "            , $2\n",
    "            , $3\n",
    "            , $4\n",
    "            , $5\n",
    "            , $6\n",
    "            , $7\n",
    "            , $8\n",
    "            , $9\n",
    "            , $10\n",
    "            , $11\n",
    "            , $12\n",
    "            , $13\n",
    "            , $14\n",
    "            , $15\n",
    "    from {stage}/psi_first_views/dev/fv_pred_munged_2022-06-23_adhoc.csv\n",
    "\n",
    "     (FILE_FORMAT => csv_v2)\n",
    "\n",
    "    '''.format(stage = stage, table_name = table_name,\n",
    "              file_name = table_name+'.csv')\n",
    "            , dbname, schema)\n",
    "\n",
    "    print ('Finish Uploading')   \n",
    "    \n",
    "    \n",
    "import io\n",
    "\n",
    "output_bucket = 'hbo-outbound-datascience-content-dev'\n",
    "key_path = 'psi_first_views/dev'\n",
    "\n",
    "Utils.read_csv_s3(output_bucket, key_path, f'fv_pred_munged_2022-06-23_adhoc.csv')\n",
    "df_pred_future_out = df_pred_future_out.reset_index()\n",
    "cvdf_to_snowflake(df_pred_future_out, 'firstview_postgl_temp')\n",
    "\n",
    "#  hbo-outbound-datascience-content-dev/psi_first_views/psi_monthly_xgb_forecast.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
